import uuid
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from confluent_kafka import SerializingProducer
from datetime import datetime
import random
import json
import time
from kafka import KafkaConsumer
from airflow.providers.amazon.aws.hooks.s3 import S3Hook# from airflow.providers.amazon.aws.hooks.s3 import S3Hook
import logging

default_args = {
    'owner': 'gauss',
    'retries': 5,
    'retry_delay': timedelta(minutes=5)
}

def fake_generate():
    return {
        "name":random.choice(['t-shirt','pantalon', 'chaussures','robe', 'jupe', 'chemise', 'short']),
        "price":int(random.uniform(1,1000)),
        "category":random.choice(['vetement hiver', 'vetement été', 'vetement automne', 'vetement printent']),
        "instock":random.choice(["true","false"]),
        "tags":random.choice(['version1','version2','version3']),
        "Descriiption":random.choice(['good for summer time','bon en hiver' 'stylé', 'confortable']),
        "Filename":random.choice(['product1.png', 'product2.png', 'product3.jpg', 'product4.jpeg', 'product5.gif'])
    }

def stream_data_from_Api_into_Kafka():
    topic = 'items_shopify_posted'
    producer= SerializingProducer({
        'bootstrap.servers': 'kafka1:29092'
    })

    curr_time = datetime.now()

    while (datetime.now() - curr_time).seconds < 180:
        try:
            posted_items = fake_generate()

            print(posted_items)

            producer.produce(topic,
                             key=posted_items['name'],
                             value=json.dumps(posted_items),
                             on_delivery=delivery_report
                             )
            producer.poll(0)

            #wait for 5 seconds before sending the next transaction
            time.sleep(2)
        except BufferError:
            print("Buffer full! Waiting...")
            time.sleep(1)
        except Exception as e:
            print(e)

def stream_data_from_Kafka_into_S3_1(bucketname, sourcekey, destinationkey):

 s3_hook = S3Hook(aws_conn_id='aws_conn')

#Read data from Kafka3 
 consumer = KafkaConsumer('users.items', bootstrap_servers='kafka1:29092')

# Apply custom Transformation and then Load S3 file
 for message in consumer:
        comment = message.value.decode('utf-8')
        s3_hook.load_string(comment, bucket_name=bucketname, Key=f'shopify.items/{message.offset}.csv')


def stream_data_from_Kafka_into_S3_2(bucketname, sourcekey, destinationkey):

 s3_hook = S3Hook(aws_conn_id='aws_conn')

#Read data from Kafka3 
 consumer = KafkaConsumer('shopify.items', bootstrap_servers='kafka3:29094')

# Apply custom Transformation and then Load S3 file
 for message in consumer:
        comment = message.value.decode('utf-8')
        s3_hook.load_string(comment, bucket_name=bucketname, Key=f'shopify.items/{message.offset}.csv')


with DAG(
         default_args=default_args,
         dag_id='user_automation',
         description='Our first dag using python operator',
         start_date=datetime(2024,  6, 17),
         schedule_interval='@daily') as dag:

    streaming_task = PythonOperator(
        task_id='stream_data_from_api',
        python_callable=stream_data_from_Api_into_Kafka
    )

   # streaming_task_1 = PythonOperator(
    #    task_id='stream_data_from_api_into_Kafka_S3',
    #    python_callable=stream_data_from_Kafka_into_S3_1
    #)

   # streaming_task_2 = PythonOperator(
    #    task_id='stream_data_from_Mongo_into_Kafka_S3',
    #    python_callable=stream_data_from_Kafka_into_S3_2
    #)
